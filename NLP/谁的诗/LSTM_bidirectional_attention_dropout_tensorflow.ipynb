{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "### author qhduan@memect.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本量：12126，测试集样本量：3032\n"
     ]
    }
   ],
   "source": [
    "from data import X_train, X_test, y_train, y_test\n",
    "from data import fit_vectorizer, fit_onehot\n",
    "from data import batch_flow, test_batch_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单个训练样本最大长度：14\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 64\n",
    "PAD = ' ' # 句子不到max_len长度时的占位符\n",
    "max_len = max(len(x) for x in X_train)\n",
    "print('单个训练样本最大长度：{}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = fit_vectorizer(X_train, embedding_size, max_len, PAD)\n",
    "onehot = fit_onehot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_steps 14\n",
      "input_size 64\n",
      "target_size 2\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "num_units = 64\n",
    "batch_size = 256\n",
    "time_steps = max_len\n",
    "input_size = embedding_size\n",
    "target_size = len(onehot.feature_indices_)\n",
    "print('time_steps', time_steps)\n",
    "print('input_size', input_size)\n",
    "print('target_size', target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 14, 64) (256, 2)\n"
     ]
    }
   ],
   "source": [
    "test_batch_flow(X_train, y_train, batch_size, vectorizer, onehot, max_len, PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [time_steps, batch_size, input_size], name='X')\n",
    "y = tf.placeholder(tf.float32, [batch_size, target_size], name='y')\n",
    "# 这里的weight相别其他实现大小*2了，因为双向LSTM的输出是双倍的，Forward一次，Backward一次\n",
    "weight = tf.Variable(tf.random_normal([time_steps * num_units * 2, target_size]), name='weight')\n",
    "bias = tf.Variable(tf.zeros([target_size]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dynamic_scope\", reuse=None):\n",
    "    X_ = tf.reshape(X, [-1, input_size])\n",
    "    # 分割为 time_steps 个数组，每个数组大小 batch_size * input_size\n",
    "    X_ = tf.split(0, time_steps, X_)\n",
    "    \n",
    "    fw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    fw_cell = tf.contrib.rnn.AttentionCellWrapper(fw_cell, 8, state_is_tuple=True)\n",
    "    fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=0.4)\n",
    "    \n",
    "    bw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    bw_cell = tf.contrib.rnn.AttentionCellWrapper(bw_cell, 8, state_is_tuple=True)\n",
    "    bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, _, _ = tf.nn.bidirectional_rnn(\n",
    "        fw_cell, bw_cell, X_, dtype=tf.float32\n",
    "    )\n",
    "    outputs = tf.reshape(outputs, [batch_size, -1])\n",
    "    pred = tf.nn.softmax(tf.add(tf.matmul(outputs, weight), bias))\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    cost = tf.reduce_mean(\n",
    "        -tf.reduce_sum(y * tf.log(pred),\n",
    "        reduction_indices=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化所有变量\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable GPU，关闭GPU支持\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 cost: 0.8274 acc: 0.6484\n",
      "epoch 1 cost: 0.7131 acc: 0.6992\n",
      "epoch 2 cost: 0.6737 acc: 0.7266\n",
      "epoch 3 cost: 0.6465 acc: 0.7344\n",
      "epoch 4 cost: 0.6320 acc: 0.7461\n",
      "epoch 5 cost: 0.6256 acc: 0.7812\n",
      "epoch 6 cost: 0.6118 acc: 0.7695\n",
      "epoch 7 cost: 0.5996 acc: 0.7617\n",
      "epoch 8 cost: 0.5963 acc: 0.7461\n",
      "epoch 9 cost: 0.5902 acc: 0.8047\n",
      "epoch 10 cost: 0.5797 acc: 0.7969\n",
      "train cost: 0.5683 acc: 0.8008\n",
      "test cost: 0.7190 acc: 0.6094\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch + 1):\n",
    "        costs = []\n",
    "        accs = []\n",
    "        for X_sample, y_sample in batch_flow(X_train, y_train, batch_size, vectorizer, onehot, max_len, PAD):\n",
    "            feeds = {X: X_sample.reshape([time_steps, batch_size, input_size]), y: y_sample}\n",
    "            sess.run(train_step, feeds)\n",
    "            c, acc = sess.run([cost, accuracy], feeds)\n",
    "            costs.append(c)\n",
    "            accs.append(acc)\n",
    "        print('epoch {} cost: {:.4f} acc: {:.4f}'.format(\n",
    "            epoch, np.mean(costs), np.mean(acc)\n",
    "        ))\n",
    "    # train\n",
    "    costs = []\n",
    "    accs = []\n",
    "    for X_sample, y_sample in batch_flow(X_train, y_train, batch_size, vectorizer, onehot, max_len, PAD):\n",
    "        feeds = {X: X_sample.reshape([time_steps, batch_size, input_size]), y: y_sample}\n",
    "        c, acc = sess.run([cost, accuracy], feeds)\n",
    "        costs.append(c)\n",
    "        accs.append(acc)\n",
    "    print('train cost: {:.4f} acc: {:.4f}'.format(np.mean(costs), np.mean(acc)))\n",
    "    # test\n",
    "    costs = []\n",
    "    accs = []\n",
    "    for X_sample, y_sample in batch_flow(X_test, y_test, batch_size, vectorizer, onehot, max_len, PAD):\n",
    "        feeds = {X: X_sample.reshape([time_steps, batch_size, input_size]), y: y_sample}\n",
    "        c, acc = sess.run([cost, accuracy], feeds)\n",
    "        costs.append(c)\n",
    "        accs.append(acc)\n",
    "    print('test cost: {:.4f} acc: {:.4f}'.format(np.mean(costs), np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
